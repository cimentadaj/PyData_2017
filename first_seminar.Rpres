<style>

bold {font-weight: bold; }

.section .reveal .state-background {
    background: white;
}

.section .reveal p {
   color: black;
   text-align:center;
   font-size: 1.8em;
}

.section .reveal h1,
.section .reveal h2 {
    color: black;
    text-align:center;
    width:100%;
}

</style>

An introduction to the tidyverse
========================================================
author: Jorge Cimentada  
date: 19th of May of 2017  
class: illustration
font-family: 'Helvetica'
width: 1800
height: 900

<div align="center">
<img src="./figures/logo_tidyverse.png" width=300 height=350>
</div>

The philosophy of the tidyverse 
========================================================
incremental: true

```{r, echo = F}
knitr::opts_chunk$set(fig.width = 15)
```

- What is the tidyverse?

- "The packages in the tidyverse share a common philosophy of data and R programming, and are designed to work together naturally." R4DS

- They've been created with the same data process in mind.

Remember R packages have been created by thousands of users without a clear structure!

The tidyverse is one attempt at unifying a philosophy of data analysis.

What is the tidyverse?
=======================================================

Yeah, the tidyverse is really just a package that installs other packages:

"The tidyverse package is designed to make it easy to install and load core packages from the tidyverse in a single command." Hadley Wickham  

```{r, eval  = F}
install.packages("tidyverse")
```

What is the tidyverse?
=======================================================

What is the tidyverse for?
- It is primarily for doing data analysis and data science.

"My goal is to make the data analysis process a pit of success" Hadley Wickham

<div align="left">
<img src="./figures/data-science.png" width=1000 height=300>
</div>

<small>http://r4ds.had.co.nz/introduction.html</small>

It covers all the important steps in the data analysis workflow. You start by importing the data, this can be a well structured csv or json file or something very messy you scraped from a website. Next, you have to tidy the data, or make it suitable for analysis and then you enter the 'interactive' section of data analysis. Here there's an interplay of creating/transforming variables, visualizing interesting patterns and try to model and explain these patterns. This process gets repeated iteratively until you reach a conclusion. That is the moment we communicate our results.

What is the tidyverse?
======================================================

<div align="center">
<img src="./figures/tidyverse_pkgs.png" width=2000 height=650>
</div>

For all packages, see [here](https://github.com/tidyverse/tidyverse)

Let's get our hands dirty!
=======================================================

```{r, eval = F}
library(tidyverse)

# Loading tidyverse: readr
# Loading tidyverse: tibble
# Loading tidyverse: tidyr
# Loading tidyverse: dplyr
# Loading tidyverse: purrr
# Loading tidyverse: ggplot2
```

- These are the main packages and the ones that are loaded automatically by the tidyverse
- Today we will talk about the core tidyverse packages, ocassionaly mentioning which other packages are useful in certain situations.

Import
========================================================

```{r, echo = F}
library(tidyverse)
```

Formats:

* csv
* semi-colon delim
* text
* fixed-width-file
* json
* xml and xlsx
* databases
* etc.. see [here](https://github.com/tidyverse/tidyverse)

The usual plain-text files can be read/written with:

read_*()
write_*()

But other formats have slightly different syntax (For example, databases or jsons).

Once you're familiar with one or two read_*() functions, you're pretty much familiar with
all other read_*() functions.

Import
========================================================

```{r, echo = F}
options(tibble.print_min = 5,
        tibble.max_extra_cols = 3)
```

Let's read our toy dataset.
```{r}
url <- "https://raw.githubusercontent.com/cimentadaj/PyData_2017/master/data/police_killings.csv"
police_killings <- read_csv(url)

# More generally:
police_killings <- read_delim(url, delim = ",")
```

read_*() functions try to guess the class of each column using a set of standard parsing functions.

Several arguments are set to usual defaults:
* Column types are guessed using the first 1000 rows (problem?)
* First row is assumed to be the header
* The delimiter is set to be equal to the \* of read_*() (comma, semi-colon, etc..)
* country-specific options are set to be US-centric, as R (dates, decimal points, timezonez)

`?locale`

Import
============================================================

The `tidyverse` contains other packages to read other types of data structure.

* Package DBI (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.

* For hierarchical data: use `jsonlite` (by Jeroen Ooms) for json, and `xml2` for XML.

* `httr` for web apis and `rvest` for web scraping

* `feather` for sharing with Python and other languages.

Tidying your data
========================================================

# Who uses data frames in R?

Data frames are, in most of the cases, the primary unit of analysis in R. It allows you to have your data in a rectangular format, with related objects occupying a row-column cell. Data frames have been around for the past 10-20 years, since S, the predecessor of R.

Tidying your data
========================================================

```{r, echo = F}
options(tibble.width = 55)
select(police_killings, name:month)
options(tibble.width = 70)
```

***

```{r, echo = F}
select(police_killings, name:month) %>%
  as.data.frame() %>%
  head(10)
```

Tidying your data
========================================================

```{r, echo = F}
options(tibble.width = 55)
select(police_killings, name:month) %>%
  print(n = 5)
options(tibble.width = 110)
```


The most obvious differences:
* Tibbles print only a reasonable number of rows (have you resarted R because of printing a massive df?)
* Each column has class printed out
* Dimensions on top of each row

Othe important differences:
* Subseting mtcars[, "mpg"] returns a tibble (no more drop = F).
* Not all functions work with tibbles, so remember to as.data.frame() your tibble

Tidying and transforming your data
========================================================
* How many Blacks were killed per state?
```{r}
# Can you understand this?
filtered_df <- filter(
    select(police_killings, state, raceethnicity),
    raceethnicity == "Black")

summarize(
  group_by(filtered_df, state),
  blacks_deaths = n())
```

Tidying and transforming your data
========================================================
* Do states with higher variance in household income (inequality) have more black deaths?
```{r}
# Can you understand this?
summarized_df <-
  arrange(
  summarize(
    group_by(police_killings, state),
    black_deaths = mean(raceethnicity == "Black", na.rm = T),
    sd_hhincome = sd(h_income, na.rm = T)),
  sd_hhincome)

top_n(summarized_df, 5, sd_hhincome)
```

The pipe
========================================================

Brief detour:

<div align="center">
<img src="./figures/pipe.png" width=800 height=530>
</div>

This is the pipe.

The pipe
============================================================

* Instead of mean(x), x %>% mean() is equivalent to mean(x)

* Just as the `|` pipe in Unix

* The pipe carries the previous result into the next function

* Best thing: you didn't have to create an object

* This mantains a logical and clean workflow

The pipe
========================================================
We could rewrite the previous as:

* How many Blacks were killed per state?
```{r, eval = F}
police_killings %>%
  select(state, raceethnicity) %>%
  filter(raceethnicity == "Black") %>%
  group_by(state) %>%
  summarize(
    black_deaths = n()
  )
```
* Do states with higher variance in household income (inequality) have more black deaths?
```{r, eval = F}
police_killings %>%
  group_by(state) %>%
  summarize(black_deaths = mean(raceethnicity == "Black", na.rm = T),
            sd_hhincome = sd(h_income, na.rm = T)) %>%
  arrange(sd_hhincome) %>%
  top_n(10, sd_hhincome)
```

Tidying and transforming your data
============================================================

`dplyr` is a package to do data manipulation.
* It contains 'verbs' which makes data manipulation very intuitive.

These are:

* Create new columns with `mutate()`
* Select variables with `select()`
* Select rows with `filter()`
* Change column names with `rename()`
* Sort by variables with `arrange()`
* Apply operations by group with `group_by()`
* Compute summary variables with `summarise()`

Delete: These functions provide the verbs for a language of data manipulation

Tidying and transforming your data
============================================================

They are so intuitive that even reading them can tell you what they do:

```{r}
police_killings %>%
  rename(ethnicity = raceethnicity) %>%
  select(gender, age, year, ethnicity, city) %>%
  mutate(year_born = age - year) %>%
  group_by(gender, ethnicity) %>%
  filter(gender == "Male") %>%
  summarise(average_age = mean(age, na.rm = T)) %>%
  arrange(average_age)
```

Can you tell what I'm doing?

Tidying and transforming your data
============================================================

Verb structure:

```{r, eval = F}

select(.data, variable_name, variable_name)

mutate(.data, new_name = contents, other_new_var = contents)

group_by(.data, variable_name, variable_name)

filter(.data, logical_statement, other_logical_statemente)

summarise(.data, new_var = contents, new_var = contets)

arrange(.data, var_to_sort_by, var_to_sort_by)
```

============================================================

# All of these functions accept and return a data frame!

Tidying and transforming your data
============================================================

Let's construct an expression ourselves.

* First, let's pipe the data frame `police_killings` and change the name of `raceethnicity` to `ethnicity` using the `rename()` verb
* Extend the pipeline to `filter()` only `Male` from `gender`
* Pipe the expression to `group_by()` the state

Tidying and transforming your data
============================================================

```{r, eval = F}
police_killings %>%
  rename(ethnicity = raceethnicity) %>%
  filter(gender == "Male") %>%
  group_by(state)
```


## What do we get back?

* Extend the pipeline by using the `summarise()` function and create the new variable `avg_black` and `avg_white` which calculates the proportion of black and white deaths for each state.
* Extend the pipeline to `arrange()` avg_black.
* Save the expression with the name `black_white_deaths`

Tidying and transforming your data
============================================================

```{r, eval = F}
black_white_deaths <-
  police_killings %>%
  rename(ethnicity = raceethnicity) %>%
  filter(gender == "Male") %>%
  group_by(state) %>%
  summarise(avg_white = mean(ethnicity == "White", na.rm = T),
            avg_black = mean(ethnicity == "Black", na.rm = T)) %>%
  arrange(avg_black)
```

Tidying and transforming your data
============================================================
`dplyr` also has a bunch of 'helper' functions that allow you to do usual transformations.

```{r}
police_killings %>%
  filter(between(age, 15, 25)) %>%
  distinct(state)
```

***

```{r}
police_killings %>%
  transmute(
    age,
    age_groups = ntile(age, 5),
    age_means = cummean(age),
    age_rank = row_number(age),
    n = n()
  )
```

For a complete list see [here](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

Tidying and transforming your data
============================================================

`dplyr` also has a series of verbs for merging datasets. These are mostly equivalent
to `merge` from `base` and `SQL` joins.

* `inner_join(x, y, by = index)` merges `y` to `x` and only retains rows that matched in both.
* `left_join(x, y, by = index)` merges `y` to `x` but retains all rows in `x`.
* `right_join(x, y, by = index)` merges `x` to `y` and retains all rows in `y`. Opposite of `left_join`.
* `full_join(x, y, by = index)` merges `x` and `y` and retains rows from both datasets

Tidying and transforming your data
============================================================
To those familiar with `base` `R` and `MySQL`

dplyr              | merge
-------------------|-------------------------------------------
`inner_join(x, y)` | `merge(x, y)`
`left_join(x, y)`  | `merge(x, y, all.x = TRUE)`
`right_join(x, y)` | `merge(x, y, all.y = TRUE)`,
`full_join(x, y)`  | `merge(x, y, all.x = TRUE, all.y = TRUE)`

dplyr                        | SQL
-----------------------------|-------------------------------------------
`inner_join(x, y, by = "z")` | `SELECT * FROM x INNER JOIN y USING (z)`
`left_join(x, y, by = "z")`  | `SELECT * FROM x LEFT OUTER JOIN y USING (z)`
`right_join(x, y, by = "z")` | `SELECT * FROM x RIGHT OUTER JOIN y USING (z)`
`full_join(x, y, by = "z")`  | `SELECT * FROM x FULL OUTER JOIN y USING (z)`

Taken from [R4DS](https://www.amazon.es/R-Data-Science-Garrett-Grolemund/dp/1491910399/ref=sr_1_1?ie=UTF8&qid=1494494889&sr=8-1&keywords=r+for+data+science), page 187.

Tidying and transforming your data
============================================================
Are monthly police deaths correlated with monthly deaths killed by the police?

```{r}
url_deaths <- "https://raw.githubusercontent.com/cimentadaj/PyData_2017/master/data/police_deaths.csv"

police_deaths <-
  read_csv(url_deaths) %>%
  filter(year == "2015") %>%
  count(month = lubridate::month(date, label = T, abbr = F), states_death)
```

```{r}
police_killings %>%
  count(state, month) %>%
  left_join(police_deaths, by = c("state" = "states_death", "month")) %>%
  # Why is state different than month?
  ungroup() %>% # Why do we ungroup()?
  summarize(correlation = cor(n.x, n.y, use = "complete.obs"))
```

Tidying and transforming your data
============================================================

Finally, tidying your data cannot be finished without talking about the main verbs from the `tidyr` package.

* `gather()` turns several columns into a stacked column.
* `spread()` turns a stacked column into several columns.
* `separate()` turns a column into several columns.
* `unite()` turns several columns into one.
* `nest()` creates a list-column.

Tidying and transforming your data
============================================================

```{r}
(stacked_df <-
  police_killings %>%
  select(state, ends_with("_income")) %>%
  gather(type_income, income, -state)) # Turns a wide dataset into a stacked column
```
***
```{r}
stacked_df %>%
  separate(type_income, c('type', 'delete'), sep = "_") %>% # separate into two
  mutate(
    type = recode(type,
                  "p" = "personal",
                  "h" = "household",
                  "comp" = "hh/comp")
  ) %>%
  select(-delete)
```


Tidying and transforming your data
============================================================
Does the average age at death differ by gender and months?

```{r}
police_killings %>% 
  unite(month_year, month, year, sep = "-") %>% # unite several columns into one
  group_by(month_year, gender) %>%
  summarize(mean_age = mean(age, na.rm = T)) %>%
  spread(gender, mean_age) # turn stacked into wide
```

Tidying and transforming your data
============================================================

Finally, there's `nest()`. `nest()` turns grouped data frame into list-columns.

```{r}
(state_nested <-
   police_killings %>%
   group_by(state) %>%
   nest())
```

This allows us to keep everything organized into compact list-columns.

Iteration and modeling
========================================================

* Loops are not used very often in R because the can be very slow
* Loops in R are slow for the same reason any interpreted language is slow: every operation carries around a lot of extra baggage.
* For that reason, we have the apply family.
* The apply family is an efficient application of for loops.

See [here](http://stackoverflow.com/questions/7142767/why-are-loops-slow-in-r) for a more elaborate answer

```{r, eval = F}
value <-1:10^3

loop_tester <- function(val) {
  empty <- 0
  for (i in seq_along(val)) empty <- c(empty, log(val[i]))
  empty
}
### Do not run at the conference
microbenchmark::microbenchmark(
  loop = loop_tester(value), # 273 miliseconds
  apply = sapply(value, function(x) log(x)) # 7 miliseconds
)
### Do not run at the conference
```

Iteration and modeling
========================================================

Instead of the `apply` family and `for()`, the `tidyverse` has the `map` family from the `purrr` package.

All functions are consistent with each other and they fit perfectly with the philosophy of the tidyverse.

* `map()` iterates over an object and **always** returns a list
* `map2()` iterates over two objects and **always** returns a list
* `map3()` iterates over three objects and **always** returns a list
* `pmap()` iterates in parallel over a list with `p` objects and **always** returns a list

All of these functions are very strict about what they return, so for each of these there's an equivalent:
* `*_chr()` returns a character vector of length 1
* `*_lgl()` returns a logical vector of length 1
* `*_dbl()` returns a double vector of length 1
* `*_int()`returns a integer vector of length 1

These are particularly useful for programming with, to avoid unexpected errors.

Iteration and modeling
========================================================

In which states does the % of blacks living explains the poverty rate better?

```{r}
(models <-
  state_nested %>% # remember this dataset?
  mutate(model1 = map(data, function(x) lm(pov ~ share_black, data = x)),
         r_square = map2_dbl(model1, data, ~ modelr::rsquare(.x, .y)),
         n = map_dbl(model1, nobs)))
```

Neat pipeline, eh?

Perfect fit because we have only two observations

Data visualization with ggplot2
========================================================

Visualizing data is often much better than tables because it allows us to see patterns that we wouldn't otherwise. 

* `ggplot2` is one of the hallmark packages of the `tidyverse`. 
* It's so widespread that it was also [implemented in `python` as well](http://ggplot.yhathq.com/).
* It's an implementation of [The Grammar of Graphics by Leland Wilkinson](https://www.amazon.com/Grammar-Graphics-Statistics-Computing/dp/0387245448)


ggplot structure:

```{r, fig.width = 15}
ggplot(data = police_killings, aes(x = as.numeric(pov), y = pop)) +
  geom_point() +
  labs(x = "Tract poverty rate", y = "Tract population")
```

Data visualization with ggplot2
========================================================

```{r, echo=T, eval = F}
ggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) + 
  <GEOM_FUNCTION>() +
  <GEOM_FUNCTION>() +
  <GEOM_FUNCTION>() +
  ...
```

* Within the `ggplot` call, `police_killings` is set as the lookup environment.
* The `geom`(metric) function specifies the type of plot.
* Each `geom` is a different layer that is superimposed over the other
* The `data` and `mapping` arguments get passed on to the `geom` functions (so each layer
will be using the same data, but that could by different).

Data visualization with ggplot2
========================================================
For example...

```{r}
police_killings$pov <- as.numeric(police_killings$pov)
police_killings$p_income <- as.numeric(police_killings$p_income)

ggplot(police_killings, aes(pov, p_income)) +
  geom_point() +
  geom_smooth()
```

We could've excluded geom_point() and only get the smoothed line.

Data visualization with ggplot2
========================================================
We could also use two different datasets on the same plot...

```{r, fig.width = 15}
other_data <-
  police_killings %>%
  group_by(nat_bucket) %>%
  summarize(max_pov = mean(pov),
            max_pincome = mean(p_income))

ggplot(police_killings, aes(pov, p_income, colour = as.factor(nat_bucket))) +
  geom_point(alpha = 0.2) +
  geom_point(data = other_data, aes(max_pov, max_pincome), size = 4)
```


Data visualization with ggplot2
========================================================
If we added `geom_smooth(method = "lm")` to the previous what would we get back? (lm simply stands for linear model, instead of the default non-linear slope)

* A slope for the underlying x-y relationship

* One slope following the relationship of the four means

* Four slopes for each `nat_bucket` group.


Data visualization with ggplot2
========================================================

```{r, fig.width = 15}
ggplot(police_killings, aes(pov, p_income, colour = as.factor(nat_bucket))) +
  geom_point(alpha = 0.2) +
  geom_point(data = other_data, aes(max_pov, max_pincome), size = 4) +
  geom_smooth(method = "lm")
```

Data visualization with ggplot2
========================================================

How many geoms (plots)?

- `geom_point()` for scatterplots
- `geom_bar()` for barplot
- `geom_histogram()`
- `geom_boxplot()`
- `geom_density()` for density plots
- `geom_line()` for line plots (usually time-series plots)
- `geom_smooth()`
- And a bunch more

[all geoms](http://sape.inf.usi.ch/quick-reference/ggplot2/geom)  
[all ggplot2 extensions](http://www.ggplot2-exts.org/gallery/)

Data visualization
========================================================
Setting visual properties

```{r, eval = F}
# colour
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop,
                           colour = p_income > mean(p_income, na.rm = T)))

# size
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop,
                           size = as.numeric(urate)))

# shape
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop,
                           shape = p_income > mean(p_income, na.rm = T)))

# alpha (transparency)
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop,
                           alpha = p_income > mean(p_income, na.rm = T)))
```

Data visualization
========================================================

All of these aesthetics can be used outside the `aes()` wrapper.

```{r, eval = F}
# colour
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop), colour = "blue")

# size
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop), size = 2)

# shape
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop), shape = 24)

# alpha (transparency)
ggplot(data = police_killings) +
  geom_point(mapping = aes(x = pov, y = pop), size = 4,alpha = 0.2)
```

Data visualization
========================================================

Aesthetics vary by `geom_*()` functions so you should have a look at the documentation before using it.

- `alpha`
- `colour`
- `fill`
- `group` to show line plots by groups
- `shape`
- `size`
- `stroke` to control the width of the lines in dots

are common ones..

Data visualization
========================================================

Let's try it out:

We want to create a scatterplot to look at the relationship between the % of black within the county and the median household income. This will help to establish whether counties with black majority are associated with lower household income.

Hints:
- x var = `as.numeric(share_black)`
- y var = `as.numeric(h_income)`
- data name = `police_killings`
- geom = `geom_point()`


Data visualization
========================================================

Answer:

```{r}
ggplot(data = police_killings) +
  geom_point(aes(x = as.numeric(share_black),
                 y = as.numeric(h_income)))
```

Data visualization
========================================================

Let's add the size of the points to be a function of the population in the county.

Hints:
- aesthetic = `size`
- variable = `pop`

Data visualization
========================================================

Answer:

```{r}
ggplot(data = police_killings) +
  geom_point(aes(x = as.numeric(share_black),
                 y = as.numeric(h_income), size = pop))
```

Too many dots! It doesn't look that nice. Let's add `alpha = 0.3`

Data visualization
========================================================
Answer:

```{r}
ggplot(data = police_killings) +
  geom_point(aes(x = as.numeric(share_black),
                 y = as.numeric(h_income),
                 size = pop),
             alpha = 0.30)
```

Data visualization
========================================================

Let's color counties based on whether they are above/below the mean unemployment rate which is 0.11:

Hint:
- aesthetic = `colour` or `color`
- variable = `as.numeric(urate) > mean(urate, na.rm = T)`

Data visualization
========================================================

Answer:

```{r}
ggplot(data = police_killings) +
  geom_point(aes(x = as.numeric(share_black),
                 y = as.numeric(h_income),
                 size = pop,
                 colour = as.numeric(urate) > mean(urate, na.rm = T)),
             alpha = 0.30)
```


Data visualization
========================================================

Let's move the `aes` argument to the `ggplot()` call because want to add another layer.

```{r, eval = F}
ggplot(data = police_killings,
       aes(x = as.numeric(share_black),
                 y = as.numeric(h_income),
                 size = pop,
                 colour = as.numeric(urate) > mean(urate, na.rm = T))) +
  geom_point(alpha = 0.30)
```


Data visualization
========================================================

Finally, let's add the a regression line for each group:

```{r}
ggplot(data = police_killings,
       aes(x = as.numeric(share_black),
           y = as.numeric(h_income),
           colour = as.numeric(urate) > mean(urate, na.rm = T))) +
  geom_point(aes(size = pop), alpha = 0.30) +
  geom_smooth(method = "lm")
```

So much done with just a few lines!

Data visualization
========================================================

It's not far fetched to say that this is truly the surface of what ggplot2 can do.
To learn ggplot2:

[R graph coobook](http://www.cookbook-r.com/Graphs/)  
[ggplot2 book by its author](http://ggplot2.org/book/)  
[R for Data Science](http://r4ds.had.co.nz/)  

Transformation, modeling and visualization
============================================================

The strength of the `tidyverse` becomes obvious when we combine all of these packages together. We've used the `dplyr` and `tidyr` tools to create a summarised dataset. This dataset is informative but we need to visualize it.

For example, let's take the `models` data from before.

```{r, echo = F}
models
```

Transformation, modeling and visualization
============================================================

Let's plot each state as the `x` axis and `r_square` as the `y` axis with the `fill` as a function of `n`.

```{r}
ggplot(models, aes(reorder(state, -r_square), r_square, fill = n)) +
  geom_col() +
  coord_flip()
```

Data transformation with dplyr
============================================================

Also from the `tidyverse`, the `broom` package turns models into data frames ready for visualization. 

* We can use the `tidy` function to get the model output into a table.

```{r}
cleaned_df <-
  police_killings %>%
  mutate(gunshot_ornot = ifelse(cause == "Gunshot", 1, 0),
         race = case_when(.$raceethnicity == "Black" ~ 1,
                          .$raceethnicity == "White" ~ 0,
                          TRUE ~ 2),
         armed_dummy = case_when(.$armed == "No" ~ 0,
                                 .$armed == "Firearm" ~ 1,
                                 TRUE ~ 2))

models_gunshot <-
  cleaned_df %>%
  split(.$race) %>%
  map(~ glm(gunshot_ornot ~ as.character(armed_dummy), data = .x, family = 'binomial'))
```

Data transformation with dplyr
============================================================
We finish by visualizing

```{r}
models_gunshot %>%
  map(broom::tidy) %>%
  enframe() %>%
  unnest() %>%
  select(name, term, estimate, std.error) %>%
  mutate(lower = estimate - std.error * 2,
         upper = estimate + std.error * 2) %>%
  ggplot(aes(reorder(term, -estimate), estimate, colour = name)) +
  geom_point() + 
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.05) +
  scale_x_discrete(labels = c("Not armed", "Firearm", "Everything else")) +
  labs(x = "Was the subject armed when killed?",
       y = "Estimate of getting killed by a gunshot")
```

Data transformation with dplyr
============================================================

We can also just look at predicted probabilities for specific groups.

```{r}
cleaned_df %>%
  split(.$race) %>%
  map(modelr::data_grid, race, armed_dummy) %>%
  map2(models_gunshot, function(x, y) {
    x[["pred"]] <- predict(y, newdata = x, type = "response");
    x
  }) %>%
  bind_rows() %>%
  ggplot(aes(as.factor(armed_dummy), pred, colour = as.factor(race))) +
  geom_point() +
  scale_colour_discrete(name = "Race", labels = c("White", "Black", "Everything else")) +
  scale_x_discrete(labels = c("Not armed", "Firearm", "Everything else")) +
  labs(x = "Was the subject armed when killed?", y = "Probs of getting killed by a gunshot")
```

Data transformation with dplyr
============================================================

```{r, eval = F}
summarized_df <- 
  read_csv(url) %>%
  mutate(gunshot_ornot = ifelse(cause == "Gunshot", 1, 0),
         race = case_when(.$raceethnicity == "White" ~ 0, .$raceethnicity == "Black" ~ 1,
                          TRUE ~ 2),
         armed_dummy = case_when(.$armed == "No" ~ 0, .$armed == "Firearm" ~ 1, TRUE ~ 2)
  ) %>% split(.$race)

models_gunshot <-
  map(summarized_df ~ glm(gunshot_ornot ~ as.character(armed_dummy), data = .x, family = 'binomial'))

summarized_df %>%
  map(modelr::data_grid, race, armed_dummy) %>%
  map2(models_gunshot, function(x, y) {
    x[["pred"]] <- predict(y, newdata = x, type = "response");
    x
  }) %>%
  bind_rows() %>%
  ggplot(aes(as.factor(armed_dummy), pred, colour = as.factor(race))) +
  geom_point() +
  scale_colour_discrete(name = "Race", labels = c("White", "Black", "Everything else")) +
  scale_x_discrete(labels = c("Not armed", "Firearm", "Everything else")) +
  labs(x = "Was the subject armed when killed?", y = "Probs of getting killed by a gunshot")
```

Data transformation with dplyr
============================================================

Benefits of this workflow:

- Intuitive verb names
- Easy to read from left to right
- No OBJECTS were created in the process
- Allows you to think about your questions rather than on programming

Data transformation with dplyr
============================================================

```{r}
exp_data <-
  police_killings %>%
  mutate(date = zoo::as.yearmon(paste(month, year, sep = "-"), format = "%b-%Y")) %>%
  select(state, date) %>%
  complete(date, state)

exp_data %>%
  count(state, date) %>%
  mutate(n_lagged = lag(n),
         increase = n - n_lagged,
         increase = ifelse(is.na(increase), n, increase)) %>%
  ggplot(aes(date, increase, colour = state, group = state)) +
  geom_point(alpha = 0.7) +
  geom_line(alpha = 0.7) +
  scale_colour_discrete(guide = F) +
  facet_wrap(~ state) +
  xlab("Date") +
  ylab("Monthly increase in deaths relative to previous month")
```

============================================================

# Thanks!
## cimentadaj@gmail.com
- If you want to be in our mailing list
- Also, send feedback or want to propose a lecture/seminar

<div align="center">
<img src="./figures/tidyverse_pkg_stickers.png" width=800 height=530>
</div>